{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is my personal notes using the MNIST dataset (covering some basic type of NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some global para\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "EPOCH = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data\n",
    "trainset = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True           \n",
    ")\n",
    "\n",
    "testset = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the data into loader, I want to see what's a picture is like, so i picked the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "#get a random picture and have a look\n",
    "batch_img, batch_label= next(iter(trainloader))\n",
    "plt.imshow(batch_img[0].squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(batch_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I want to see each item inside the loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_img.shape\n",
    "#And see the shape: 64 imgs in a batch, 1 channel, 28 x 28 size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we start with a simple neural network first. It is a two-layer linear structure. To fit the 28x28 image inside, we need to flatten it to 1 dimension, and we classify it from then on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try with a neural network with 2 linear layers first, will be used on flattened image set\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNetwork, self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(784,256) #can tweak the paras to see if there are better results\n",
    "        self.linear2=nn.Linear(256,10) #ten categories\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x= F.relu(self.linear1(x))\n",
    "        x= F.log_softmax(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Temp\\ipykernel_12904\\3852790194.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x= F.log_softmax(self.linear2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0 Accuracy: tensor(0.8667, device='cuda:0')\n",
      "Test: 1 Accuracy: tensor(0.8943, device='cuda:0')\n",
      "Test: 2 Accuracy: tensor(0.9024, device='cuda:0')\n",
      "Test: 3 Accuracy: tensor(0.9108, device='cuda:0')\n",
      "Test: 4 Accuracy: tensor(0.9155, device='cuda:0')\n",
      "Test: 5 Accuracy: tensor(0.9208, device='cuda:0')\n",
      "Test: 6 Accuracy: tensor(0.9242, device='cuda:0')\n",
      "Test: 7 Accuracy: tensor(0.9270, device='cuda:0')\n",
      "Test: 8 Accuracy: tensor(0.9301, device='cuda:0')\n",
      "Test: 9 Accuracy: tensor(0.9326, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "model = SimpleNeuralNetwork().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LR)\n",
    "\n",
    "#training\n",
    "def train(epoch_num):\n",
    "  model.train()\n",
    "  for batch_num,(x_train,y_train) in enumerate(trainloader):\n",
    "    optimizer.zero_grad()\n",
    "    #calculate output\n",
    "    output = model(x_train.to(device))\n",
    " \n",
    "    #calculate loss\n",
    "    loss = F.nll_loss(output, y_train.to(device))\n",
    "    \n",
    "    #backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test(epoch_num):\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for x_test, y_test in testloader:\n",
    "      output = model(x_test.to(device))\n",
    "      prediction = output.data.max(1, keepdim=True)[1]\n",
    "      correct += prediction.eq(y_test.to(device).data.view_as(prediction)).sum()\n",
    "  print(\"Test: \"+ str(epoch_num) + \" Accuracy: \"+str(correct/len(testloader.dataset)))\n",
    "  \n",
    "for i in range(EPOCH):\n",
    "  train(i)\n",
    "  test(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the accuracy is pretty good, at around 90% for only 10 epochs (sorry it takes quite a while to run for more epochs, so I'll only use 10)\n",
    "\n",
    "Now let's see if we want to retain the spatial structure, i.e. know the position of pixels in a 2d way instead of simple flattening.\n",
    "\n",
    "We will use a CNN, and I build a rough one with 2 conv layers, and we extract the \"key info\" into a flattened tensor and pass it to a linear layer for it to categorize into 10 categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems flattened MNIST is pretty good, but what if we want to retain spatial info?\n",
    "#So we define CNN\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(1,5, kernel_size=5) #assume in 1 channel, out 5 channel, and it is a 5,5 square filter\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.convdrop = nn.Dropout2d()\n",
    "        self.linear1=nn.Linear(160,10)#flattened shape should be of length 160\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.relu(F.max_pool2d(self.conv1(x),2))#using max pool because it is kinda the 'brightest' spot\n",
    "        x= F.relu(F.max_pool2d(self.convdrop(self.conv2(x)),2))\n",
    "        x = self.flatten(x) #need to flatten it to pass into the linear layer\n",
    "        x=F.relu(self.linear1(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Temp\\ipykernel_12904\\2296799989.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch : 0 Loss: 2.2981276512145996\n",
      "Train batch : 100 Loss: 2.279198408126831\n",
      "Train batch : 200 Loss: 2.265749931335449\n",
      "Train batch : 300 Loss: 2.2190499305725098\n",
      "Train batch : 400 Loss: 2.0052578449249268\n",
      "Train batch : 500 Loss: 1.4925092458724976\n",
      "Train batch : 600 Loss: 1.493431568145752\n",
      "Train batch : 700 Loss: 1.1589534282684326\n",
      "Train batch : 800 Loss: 1.0297698974609375\n",
      "Train batch : 900 Loss: 1.0617928504943848\n",
      "Test: 0 Accuracy: tensor(0.7082, device='cuda:0')\n",
      "Train batch : 0 Loss: 1.0730575323104858\n",
      "Train batch : 100 Loss: 1.25726318359375\n",
      "Train batch : 200 Loss: 1.0652964115142822\n",
      "Train batch : 300 Loss: 1.2147197723388672\n",
      "Train batch : 400 Loss: 0.7091343998908997\n",
      "Train batch : 500 Loss: 0.694392204284668\n",
      "Train batch : 600 Loss: 0.5129460692405701\n",
      "Train batch : 700 Loss: 0.44637537002563477\n",
      "Train batch : 800 Loss: 0.5419738292694092\n",
      "Train batch : 900 Loss: 0.4966060221195221\n",
      "Test: 1 Accuracy: tensor(0.8965, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.4919413924217224\n",
      "Train batch : 100 Loss: 0.5062481760978699\n",
      "Train batch : 200 Loss: 0.35354548692703247\n",
      "Train batch : 300 Loss: 0.3223651051521301\n",
      "Train batch : 400 Loss: 0.3545784652233124\n",
      "Train batch : 500 Loss: 0.3818627595901489\n",
      "Train batch : 600 Loss: 0.493882417678833\n",
      "Train batch : 700 Loss: 0.38164687156677246\n",
      "Train batch : 800 Loss: 0.7312498688697815\n",
      "Train batch : 900 Loss: 0.25293251872062683\n",
      "Test: 2 Accuracy: tensor(0.9303, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.5310673713684082\n",
      "Train batch : 100 Loss: 0.36429664492607117\n",
      "Train batch : 200 Loss: 0.3340522050857544\n",
      "Train batch : 300 Loss: 0.2754315435886383\n",
      "Train batch : 400 Loss: 0.2453596144914627\n",
      "Train batch : 500 Loss: 0.2657744586467743\n",
      "Train batch : 600 Loss: 0.6473606824874878\n",
      "Train batch : 700 Loss: 0.5489199757575989\n",
      "Train batch : 800 Loss: 0.22795359790325165\n",
      "Train batch : 900 Loss: 0.2607402801513672\n",
      "Test: 3 Accuracy: tensor(0.9409, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.3853759467601776\n",
      "Train batch : 100 Loss: 0.25334855914115906\n",
      "Train batch : 200 Loss: 0.36601483821868896\n",
      "Train batch : 300 Loss: 0.345426470041275\n",
      "Train batch : 400 Loss: 0.3135344684123993\n",
      "Train batch : 500 Loss: 0.3645254969596863\n",
      "Train batch : 600 Loss: 0.20855748653411865\n",
      "Train batch : 700 Loss: 0.3532451391220093\n",
      "Train batch : 800 Loss: 0.430819571018219\n",
      "Train batch : 900 Loss: 0.26788330078125\n",
      "Test: 4 Accuracy: tensor(0.9459, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.3371466398239136\n",
      "Train batch : 100 Loss: 0.2939833998680115\n",
      "Train batch : 200 Loss: 0.26699355244636536\n",
      "Train batch : 300 Loss: 0.33051639795303345\n",
      "Train batch : 400 Loss: 0.17133653163909912\n",
      "Train batch : 500 Loss: 0.2623422145843506\n",
      "Train batch : 600 Loss: 0.33259162306785583\n",
      "Train batch : 700 Loss: 0.3373599052429199\n",
      "Train batch : 800 Loss: 0.4359320104122162\n",
      "Train batch : 900 Loss: 0.253194659948349\n",
      "Test: 5 Accuracy: tensor(0.9484, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.3674705922603607\n",
      "Train batch : 100 Loss: 0.2489561140537262\n",
      "Train batch : 200 Loss: 0.2643028795719147\n",
      "Train batch : 300 Loss: 0.2339974194765091\n",
      "Train batch : 400 Loss: 0.3043285012245178\n",
      "Train batch : 500 Loss: 0.20501558482646942\n",
      "Train batch : 600 Loss: 0.21486790478229523\n",
      "Train batch : 700 Loss: 0.2889925539493561\n",
      "Train batch : 800 Loss: 0.2756374776363373\n",
      "Train batch : 900 Loss: 0.23256295919418335\n",
      "Test: 6 Accuracy: tensor(0.9561, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.22890695929527283\n",
      "Train batch : 100 Loss: 0.22812192142009735\n",
      "Train batch : 200 Loss: 0.22048872709274292\n",
      "Train batch : 300 Loss: 0.17727157473564148\n",
      "Train batch : 400 Loss: 0.18289418518543243\n",
      "Train batch : 500 Loss: 0.16937799751758575\n",
      "Train batch : 600 Loss: 0.32352951169013977\n",
      "Train batch : 700 Loss: 0.18634316325187683\n",
      "Train batch : 800 Loss: 0.24691452085971832\n",
      "Train batch : 900 Loss: 0.3771505653858185\n",
      "Test: 7 Accuracy: tensor(0.9574, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.30502602458000183\n",
      "Train batch : 100 Loss: 0.23902343213558197\n",
      "Train batch : 200 Loss: 0.3130873739719391\n",
      "Train batch : 300 Loss: 0.12122538685798645\n",
      "Train batch : 400 Loss: 0.1318311095237732\n",
      "Train batch : 500 Loss: 0.3348114490509033\n",
      "Train batch : 600 Loss: 0.306084007024765\n",
      "Train batch : 700 Loss: 0.311056911945343\n",
      "Train batch : 800 Loss: 0.2699795067310333\n",
      "Train batch : 900 Loss: 0.3716616928577423\n",
      "Test: 8 Accuracy: tensor(0.9613, device='cuda:0')\n",
      "Train batch : 0 Loss: 0.3391358554363251\n",
      "Train batch : 100 Loss: 0.22029991447925568\n",
      "Train batch : 200 Loss: 0.1774398535490036\n",
      "Train batch : 300 Loss: 0.24944651126861572\n",
      "Train batch : 400 Loss: 0.2770244777202606\n",
      "Train batch : 500 Loss: 0.34690359234809875\n",
      "Train batch : 600 Loss: 0.08816757053136826\n",
      "Train batch : 700 Loss: 0.16766993701457977\n",
      "Train batch : 800 Loss: 0.19132958352565765\n",
      "Train batch : 900 Loss: 0.35903406143188477\n",
      "Test: 9 Accuracy: tensor(0.9628, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model_cnn = CNN().to(device)\n",
    "optimizer = torch.optim.SGD(model_cnn.parameters(),lr=LR)\n",
    "\n",
    "#training\n",
    "def train(epoch_num):\n",
    "  model_cnn.train()\n",
    "  for batch_num,(x_train,y_train) in enumerate(trainloader):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #calculate output\n",
    "    output = model_cnn(x_train.to(device))\n",
    " \n",
    "    #calculate loss\n",
    "    loss = F.nll_loss(output, y_train.to(device))\n",
    "   \n",
    "    #backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #train acc\n",
    "    if batch_num % 100 == 0:\n",
    "      print(\"Train batch : \"+ str(batch_num) + \" Loss: \"+str(loss.item()))\n",
    "\n",
    "def test(epoch_num):\n",
    "  model_cnn.eval()\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for x_test, y_test in testloader:\n",
    "      output = model_cnn(x_test.to(device))\n",
    "      prediction = output.data.max(1, keepdim=True)[1]\n",
    "      correct += prediction.eq(y_test.to(device).data.view_as(prediction)).sum()\n",
    "  print(\"Test: \"+ str(epoch_num) + \" Accuracy: \"+str(correct/len(testloader.dataset)))\n",
    "  \n",
    "for i in range(EPOCH):\n",
    "  train(i)\n",
    "  test(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results look good too. of course, we can change the parameters to get better accuracy, but for now we'll leave it like that and try our next model, GAN.\n",
    "\n",
    "Since GAN is about generating an image, perhaps we should use image for 1 number instead. Since I like number 5 I'll just pick that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [idx for idx, target in enumerate(trainset.targets) if target == 5]\n",
    "trainloader_5 = torch.utils.data.DataLoader(Subset(trainset, indices),\n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         drop_last=True)\n",
    "\n",
    "indices = [idx for idx, target in enumerate(testset.targets) if target == 5]\n",
    "testloader_5 = torch.utils.data.DataLoader(Subset(trainset, indices),\n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN8klEQVR4nO3df6jVdZ7H8ddrbfojxzI39iZOrWOEUdE6i9nSyjYRTj8o7FYMIzQ0JDl/JDSwyIb7xxSLIVu6rBSDDtXYMus0UJHFMNVm5S6BdDMrs21qoxjlphtmmv1a9b1/3K9xp+75nOs53/PD+34+4HDO+b7P93zffPHl99f53o8jQgAmvj/rdQMAuoOwA0kQdiAJwg4kQdiBJE7o5sJsc+of6LCI8FjT29qy277C9lu237F9ezvfBaCz3Op1dtuTJP1B0gJJOyW9JGlRROwozMOWHeiwTmzZ50l6JyLejYgvJf1G0sI2vg9AB7UT9hmS/jjq/c5q2p+wvcT2kO2hNpYFoE0dP0EXEeskrZPYjQd6qZ0t+y5JZ4x6/51qGoA+1E7YX5J0tu3v2j5R0o8kbaynLQB1a3k3PiIO2V4q6SlJkyQ9EBFv1NYZgFq1fOmtpYVxzA50XEd+VAPg+EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEi0P2Yzjw6RJk4r1U045paPLX7p0acPaSSedVJx39uzZxfqtt95arN9zzz0Na4sWLSrO+/nnnxfrK1euLNbvvPPOYr0X2gq77fckHZB0WNKhiJhbR1MA6lfHlv3SiPiwhu8B0EEcswNJtBv2kPS07ZdtLxnrA7aX2B6yPdTmsgC0od3d+PkRscv2X0h6xvZ/R8Tm0R+IiHWS1kmS7WhzeQBa1NaWPSJ2Vc97JD0maV4dTQGoX8thtz3Z9pSjryX9QNL2uhoDUK92duMHJD1m++j3/HtE/L6WriaYM888s1g/8cQTi/WLL764WJ8/f37D2tSpU4vzXn/99cV6L+3cubNYX7NmTbE+ODjYsHbgwIHivK+++mqx/sILLxTr/ajlsEfEu5L+qsZeAHQQl96AJAg7kARhB5Ig7EAShB1IwhHd+1HbRP0F3Zw5c4r1TZs2Feudvs20Xx05cqRYv/nmm4v1Tz75pOVlDw8PF+sfffRRsf7WW2+1vOxOiwiPNZ0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX2GkybNq1Y37JlS7E+a9asOtupVbPe9+3bV6xfeumlDWtffvllcd6svz9oF9fZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJhmyuwd69e4v1ZcuWFetXX311sf7KK68U683+pHLJtm3bivUFCxYU6wcPHizWzzvvvIa12267rTgv6sWWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H72PnDyyScX682GF167dm3D2uLFi4vz3njjjcX6hg0binX0n5bvZ7f9gO09trePmjbN9jO2366eT62zWQD1G89u/K8kXfG1abdLejYizpb0bPUeQB9rGvaI2Czp678HXShpffV6vaRr620LQN1a/W38QEQcHSzrA0kDjT5oe4mkJS0uB0BN2r4RJiKidOItItZJWidxgg7opVYvve22PV2Squc99bUEoBNaDftGSTdVr2+S9Hg97QDolKa78bY3SPq+pNNs75T0c0krJf3W9mJJ70v6YSebnOj279/f1vwff/xxy/PecsstxfrDDz9crDcbYx39o2nYI2JRg9JlNfcCoIP4uSyQBGEHkiDsQBKEHUiCsANJcIvrBDB58uSGtSeeeKI47yWXXFKsX3nllcX6008/Xayj+xiyGUiOsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BHfWWWcV61u3bi3W9+3bV6w/99xzxfrQ0FDD2n333Vect5v/NicSrrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZ09ucHCwWH/wwQeL9SlTprS87OXLlxfrDz30ULE+PDxcrGfFdXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSILr7Cg6//zzi/XVq1cX65dd1vpgv2vXri3WV6xYUazv2rWr5WUfz1q+zm77Adt7bG8fNe0O27tsb6seV9XZLID6jWc3/leSrhhj+r9ExJzq8bt62wJQt6Zhj4jNkvZ2oRcAHdTOCbqltl+rdvNPbfQh20tsD9lu/MfIAHRcq2H/haSzJM2RNCxpVaMPRsS6iJgbEXNbXBaAGrQU9ojYHRGHI+KIpF9KmldvWwDq1lLYbU8f9XZQ0vZGnwXQH5peZ7e9QdL3JZ0mabekn1fv50gKSe9J+mlENL25mOvsE8/UqVOL9WuuuaZhrdm98vaYl4u/smnTpmJ9wYIFxfpE1eg6+wnjmHHRGJPvb7sjAF3Fz2WBJAg7kARhB5Ig7EAShB1Igltc0TNffPFFsX7CCeWLRYcOHSrWL7/88oa1559/vjjv8Yw/JQ0kR9iBJAg7kARhB5Ig7EAShB1IgrADSTS96w25XXDBBcX6DTfcUKxfeOGFDWvNrqM3s2PHjmJ98+bNbX3/RMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Dr7BDd79uxifenSpcX6ddddV6yffvrpx9zTeB0+fLhYHx4u//XyI0eO1NnOcY8tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwXX240Cza9mLFo010O6IZtfRZ86c2UpLtRgaGirWV6xYUaxv3LixznYmvKZbdttn2H7O9g7bb9i+rZo+zfYztt+unk/tfLsAWjWe3fhDkv4+Is6V9DeSbrV9rqTbJT0bEWdLerZ6D6BPNQ17RAxHxNbq9QFJb0qaIWmhpPXVx9ZLurZDPQKowTEds9ueKel7krZIGoiIoz9O/kDSQIN5lkha0kaPAGow7rPxtr8t6RFJP4uI/aNrMTI65JiDNkbEuoiYGxFz2+oUQFvGFXbb39JI0H8dEY9Wk3fbnl7Vp0va05kWAdSh6W68bUu6X9KbEbF6VGmjpJskrayeH+9IhxPAwMCYRzhfOffcc4v1e++9t1g/55xzjrmnumzZsqVYv/vuuxvWHn+8/E+GW1TrNZ5j9r+V9GNJr9veVk1brpGQ/9b2YknvS/phRzoEUIumYY+I/5I05uDuki6rtx0AncLPZYEkCDuQBGEHkiDsQBKEHUiCW1zHadq0aQ1ra9euLc47Z86cYn3WrFmttFSLF198sVhftWpVsf7UU08V65999tkx94TOYMsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkuc5+0UUXFevLli0r1ufNm9ewNmPGjJZ6qsunn37asLZmzZrivHfddVexfvDgwZZ6Qv9hyw4kQdiBJAg7kARhB5Ig7EAShB1IgrADSaS5zj44ONhWvR07duwo1p988sli/dChQ8V66Z7zffv2FedFHmzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T5A/YZkh6SNCApJK2LiH+1fYekWyT9b/XR5RHxuybfVV4YgLZFxJijLo8n7NMlTY+IrbanSHpZ0rUaGY/9k4i4Z7xNEHag8xqFfTzjsw9LGq5eH7D9pqTe/mkWAMfsmI7Zbc+U9D1JW6pJS22/ZvsB26c2mGeJ7SHbQ+21CqAdTXfjv/qg/W1JL0haERGP2h6Q9KFGjuP/SSO7+jc3+Q5244EOa/mYXZJsf0vSk5KeiojVY9RnSnoyIs5v8j2EHeiwRmFvuhtv25Lul/Tm6KBXJ+6OGpS0vd0mAXTOeM7Gz5f0n5Jel3Skmrxc0iJJczSyG/+epJ9WJ/NK38WWHeiwtnbj60LYgc5reTcewMRA2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLbQzZ/KOn9Ue9Pq6b1o37trV/7kuitVXX29peNCl29n/0bC7eHImJuzxoo6Nfe+rUvid5a1a3e2I0HkiDsQBK9Dvu6Hi+/pF9769e+JHprVVd66+kxO4Du6fWWHUCXEHYgiZ6E3fYVtt+y/Y7t23vRQyO237P9uu1tvR6frhpDb4/t7aOmTbP9jO23q+cxx9jrUW932N5Vrbtttq/qUW9n2H7O9g7bb9i+rZre03VX6Ksr663rx+y2J0n6g6QFknZKeknSoojY0dVGGrD9nqS5EdHzH2DY/jtJn0h66OjQWrb/WdLeiFhZ/Ud5akT8Q5/0doeOcRjvDvXWaJjxn6iH667O4c9b0Yst+zxJ70TEuxHxpaTfSFrYgz76XkRslrT3a5MXSlpfvV6vkX8sXdegt74QEcMRsbV6fUDS0WHGe7ruCn11RS/CPkPSH0e936n+Gu89JD1t+2XbS3rdzBgGRg2z9YGkgV42M4amw3h309eGGe+bddfK8Oft4gTdN82PiL+WdKWkW6vd1b4UI8dg/XTt9BeSztLIGIDDklb1splqmPFHJP0sIvaPrvVy3Y3RV1fWWy/CvkvSGaPef6ea1hciYlf1vEfSYxo57Ognu4+OoFs97+lxP1+JiN0RcTgijkj6pXq47qphxh+R9OuIeLSa3PN1N1Zf3VpvvQj7S5LOtv1d2ydK+pGkjT3o4xtsT65OnMj2ZEk/UP8NRb1R0k3V65skPd7DXv5Evwzj3WiYcfV43fV8+POI6PpD0lUaOSP/P5L+sRc9NOhrlqRXq8cbve5N0gaN7Nb9n0bObSyW9OeSnpX0tqT/kDStj3r7N40M7f2aRoI1vUe9zdfILvprkrZVj6t6ve4KfXVlvfFzWSAJTtABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/DyJ7caZa7LphAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get a random picture and have a look\n",
    "batch_img, batch_label = next(iter(trainloader_5))\n",
    "plt.imshow(batch_img[0].squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator takes in the whole 784 numbers and use 2 layers to divide into 2 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear1=nn.Linear(784,256) #can tweak the paras to see if there are better results\n",
    "        self.linear2=nn.Linear(256,2) #ten categories\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.flatten(x)\n",
    "        x= F.relu(self.linear1(x))\n",
    "        x= F.log_softmax(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gnerator takes in random noise to get the 784 numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator (nn.Module):\n",
    "    def __init__(self, inpute_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1=nn.Linear(input_size,256) #can tweak the paras to see if there are better results\n",
    "        self.linear2=nn.Linear(256,784) #ten categories\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= F.relu(self.linear1(x))\n",
    "        x= torch.tanh(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created some hyper parameter below. The 100 is the input noise size, and epoch is changed to 100 to get a better picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper para\n",
    "input_size = 100\n",
    "EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charl\\AppData\\Local\\Temp\\ipykernel_12904\\1753854258.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x= F.log_softmax(self.linear2(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 1.080370545387268 D: 0.4720352292060852\n",
      "G: 0.812414824962616 D: 0.7685874104499817\n",
      "G: 0.829698920249939 D: 0.8583457469940186\n",
      "G: 0.8324255347251892 D: 0.9721237421035767\n",
      "G: 0.9152796864509583 D: 0.9770674109458923\n",
      "G: 1.1318325996398926 D: 0.7996824979782104\n",
      "G: 1.4470990896224976 D: 0.5644732713699341\n",
      "G: 1.7209727764129639 D: 0.37917831540107727\n",
      "G: 1.7703897953033447 D: 0.3382510542869568\n",
      "G: 1.933081865310669 D: 0.3372156620025635\n",
      "G: 1.8876174688339233 D: 0.38473933935165405\n",
      "G: 1.7420073747634888 D: 0.41964107751846313\n",
      "G: 1.575890064239502 D: 0.7849604487419128\n",
      "G: 1.521875023841858 D: 0.5306041836738586\n",
      "G: 1.6665974855422974 D: 0.47660893201828003\n",
      "G: 1.6999620199203491 D: 0.4668906033039093\n",
      "G: 1.6899415254592896 D: 0.3963555097579956\n",
      "G: 1.8730520009994507 D: 0.4167863130569458\n",
      "G: 1.8741395473480225 D: 0.4457424283027649\n",
      "G: 2.3164610862731934 D: 0.323199987411499\n",
      "G: 1.7308251857757568 D: 0.7450475692749023\n",
      "G: 1.6813454627990723 D: 0.669833242893219\n",
      "G: 1.8546912670135498 D: 0.511181116104126\n",
      "G: 1.7884563207626343 D: 0.5305050611495972\n",
      "G: 2.1554954051971436 D: 0.3712606132030487\n",
      "G: 2.2816216945648193 D: 0.5267691016197205\n",
      "G: 1.8454197645187378 D: 0.6420224905014038\n",
      "G: 2.059377431869507 D: 0.6370536088943481\n",
      "G: 1.7357609272003174 D: 0.665360152721405\n",
      "G: 1.9194633960723877 D: 0.5408285856246948\n",
      "G: 1.8129193782806396 D: 0.6135274171829224\n",
      "G: 1.6111068725585938 D: 1.0774157047271729\n",
      "G: 2.062412977218628 D: 0.4659504294395447\n",
      "G: 2.0891995429992676 D: 0.830817699432373\n",
      "G: 2.1908841133117676 D: 0.49181634187698364\n",
      "G: 1.7711576223373413 D: 0.5798284411430359\n",
      "G: 1.828497290611267 D: 0.6479454040527344\n",
      "G: 2.090494155883789 D: 0.5200644731521606\n",
      "G: 1.6790779829025269 D: 1.0598727464675903\n",
      "G: 2.309800386428833 D: 0.5007020235061646\n",
      "G: 1.6078425645828247 D: 0.7590726613998413\n",
      "G: 1.8983107805252075 D: 0.4962536692619324\n",
      "G: 1.7211663722991943 D: 0.7025622129440308\n",
      "G: 1.634087085723877 D: 0.7004852294921875\n",
      "G: 1.8688308000564575 D: 0.6359292268753052\n",
      "G: 1.873630404472351 D: 0.5955728888511658\n",
      "G: 1.398503065109253 D: 0.7646258473396301\n",
      "G: 1.9316951036453247 D: 0.6675300598144531\n",
      "G: 1.8484655618667603 D: 0.6735304594039917\n",
      "G: 1.9767189025878906 D: 0.5606343746185303\n",
      "G: 1.7763334512710571 D: 0.6551937460899353\n",
      "G: 2.1086478233337402 D: 0.5103789567947388\n",
      "G: 2.227123975753784 D: 0.5816950798034668\n",
      "G: 2.185530662536621 D: 0.6318080425262451\n",
      "G: 1.931511640548706 D: 0.4949864149093628\n",
      "G: 1.685483694076538 D: 0.6293959617614746\n",
      "G: 2.073122024536133 D: 0.6371315717697144\n",
      "G: 2.030442476272583 D: 0.6043002605438232\n",
      "G: 2.105363368988037 D: 0.5303514003753662\n",
      "G: 2.357849597930908 D: 0.5913339257240295\n",
      "G: 2.170215606689453 D: 0.665766716003418\n",
      "G: 2.1266438961029053 D: 0.7020167112350464\n",
      "G: 1.8670107126235962 D: 0.5845321416854858\n",
      "G: 2.265951156616211 D: 0.5462549924850464\n",
      "G: 1.9155821800231934 D: 0.5928546190261841\n",
      "G: 1.9175207614898682 D: 0.6764857172966003\n",
      "G: 2.0866963863372803 D: 0.6229520440101624\n",
      "G: 2.1083993911743164 D: 0.5659177899360657\n",
      "G: 2.1412856578826904 D: 0.5977931618690491\n",
      "G: 1.8439387083053589 D: 0.6573407649993896\n",
      "G: 2.2012150287628174 D: 0.6536579728126526\n",
      "G: 1.7443933486938477 D: 0.6487483978271484\n",
      "G: 2.1398227214813232 D: 0.6549204587936401\n",
      "G: 2.0330309867858887 D: 0.6792427897453308\n",
      "G: 1.7731835842132568 D: 0.6467849016189575\n",
      "G: 2.0057003498077393 D: 0.6338728666305542\n",
      "G: 2.0678491592407227 D: 0.6425283551216125\n",
      "G: 1.936431884765625 D: 0.7781835794448853\n",
      "G: 1.6443113088607788 D: 0.655604362487793\n",
      "G: 2.1702651977539062 D: 0.691102147102356\n",
      "G: 1.8211207389831543 D: 0.6882079839706421\n",
      "G: 1.692092776298523 D: 0.6074033975601196\n",
      "G: 2.1276910305023193 D: 0.7309970855712891\n",
      "G: 1.9752684831619263 D: 0.7139561176300049\n",
      "G: 1.9856934547424316 D: 0.6948728561401367\n",
      "G: 2.0370569229125977 D: 0.7120335102081299\n",
      "G: 1.968424916267395 D: 0.6663733124732971\n",
      "G: 1.7975120544433594 D: 0.5932928323745728\n",
      "G: 1.7538416385650635 D: 0.5874086618423462\n",
      "G: 1.925713062286377 D: 0.6421000957489014\n",
      "G: 1.8290376663208008 D: 0.6300605535507202\n",
      "G: 1.5457301139831543 D: 0.6038401126861572\n",
      "G: 1.8534330129623413 D: 0.6043857336044312\n",
      "G: 1.4914839267730713 D: 0.6041630506515503\n",
      "G: 1.784368872642517 D: 0.6979344487190247\n",
      "G: 1.7698029279708862 D: 0.6269413232803345\n",
      "G: 2.1699061393737793 D: 0.5748356580734253\n",
      "G: 1.8744311332702637 D: 0.6617797613143921\n",
      "G: 1.872983455657959 D: 0.6645772457122803\n",
      "G: 1.7148813009262085 D: 0.5823967456817627\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "D=Discriminator()\n",
    "G=Generator(input_size)\n",
    "\n",
    "#define optim\n",
    "D_optimizer = torch.optim.SGD(D.parameters(), LR)\n",
    "G_optimizer = torch.optim.SGD(G.parameters(), LR)\n",
    "\n",
    "def train():\n",
    "  D.train()\n",
    "  G.train()\n",
    "  for item in trainloader_5:\n",
    "    \n",
    "    #train D first\n",
    "    D_optimizer.zero_grad()\n",
    "    D_real = D(item[0])\n",
    "    D_real_loss = F.nll_loss(D_real, torch.from_numpy(np.ones(BATCH_SIZE)).type(torch. int64))\n",
    "    \n",
    "    #get some random noise\n",
    "    z = np.random.uniform(-1, 1, size=(BATCH_SIZE, input_size))\n",
    "    z = torch.from_numpy(z).float()\n",
    "    \n",
    "    fake_images = G(z)\n",
    "        \n",
    "    # Compute the discriminator losses on fake images        \n",
    "    D_fake = D(fake_images)\n",
    "    D_fake_loss = F.nll_loss(D_fake, torch.from_numpy(np.zeros(BATCH_SIZE)).type(torch. int64))\n",
    "    \n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "    \n",
    "    \n",
    "    #train G next\n",
    "    G_optimizer.zero_grad()\n",
    "    z = np.random.uniform(-1, 1, size=(BATCH_SIZE, input_size))\n",
    "    z = torch.from_numpy(z).float()\n",
    "    fake_images = G(z)\n",
    "    D_fake = D(fake_images)\n",
    "    G_loss = F.nll_loss(D_fake, torch.from_numpy(np.ones(BATCH_SIZE)).type(torch. int64))\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "  \n",
    "  print(\"G: \"+str(G_loss.item())+\" D: \"+str(D_loss.item()))\n",
    "\n",
    "for i in range(EPOCH):\n",
    "  train()\n",
    "  G.eval() # eval mode for generating samples\n",
    "  G.train()\n",
    "  \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below is the result (I picked the first one generated)! As we can see it is not perfect yet with the kinda grey background, but the '5' shape is rather distinct. With more epochs and some tuning of the architecture, it is possible to achieve a better outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input =  np.random.uniform(-1, 1, size=(BATCH_SIZE, input_size))\n",
    "input = torch.from_numpy(input).float()\n",
    "img_set = G(input)\n",
    "img=img_set[0]\n",
    "img=img.view(28,28).detach().numpy()\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6213e0af081d56db034f795ff49d96980936e2ae540dda57fc6c3cbea6a5fc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
